{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNucB9Eb6nyERehBvISDs7C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikaru55/DocumentGPT/blob/main/Shift.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# 환경 파라미터\n",
        "NUM_DAYS = 30\n",
        "NUM_PERSONNEL = 10\n",
        "MAX_CONSECUTIVE_OFF = 4\n",
        "REQUIRED_OFF = 12  # 고정 휴무일\n",
        "MIN_PD_DAYS = 4   # 최소 PD 근무일수\n",
        "\n",
        "class DutySchedulerEnv:\n",
        "    def __init__(self, cq_schedule, leave_schedule):\n",
        "        self.current_day = 0\n",
        "        self.personnel = {\n",
        "            i: {\n",
        "                'off_count': 0,\n",
        "                'rd_count': 0,    # RD 근무 일수\n",
        "                'pd_count': 0,    # PD 근무 일수\n",
        "                'last_off_days': 0,\n",
        "                'cq_next_day_off': False\n",
        "            } for i in range(NUM_PERSONNEL)\n",
        "        }\n",
        "        self.cq_schedule = cq_schedule\n",
        "        self.leave_schedule = leave_schedule\n",
        "        self.daily_duties = []  # 일별 근무 기록\n",
        "\n",
        "    def get_state(self):\n",
        "        state = []\n",
        "        for p in self.personnel.values():\n",
        "            state.extend([\n",
        "                p['off_count'],\n",
        "                p['rd_count'],\n",
        "                p['pd_count'],\n",
        "                p['last_off_days'],\n",
        "                int(p['cq_next_day_off'])\n",
        "            ])\n",
        "        return np.array(state + [self.current_day], dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_day = 0\n",
        "        for p in self.personnel.values():\n",
        "            p.update({\n",
        "                'off_count': 0,\n",
        "                'rd_count': 0,\n",
        "                'pd_count': 0,\n",
        "                'last_off_days': 0,\n",
        "                'cq_next_day_off': False\n",
        "            })\n",
        "        self.daily_duties = []\n",
        "        return self.get_state()\n",
        "\n",
        "    def _action_to_schedule(self, action):\n",
        "        rd_personnel = []\n",
        "        pd_personnel = []\n",
        "        cq_personnel = []\n",
        "\n",
        "        # 상위 10비트: RD, 중간 10비트: PD, 하위 10비트: CQ\n",
        "        for i in range(NUM_PERSONNEL):\n",
        "            if action & (1 << i):\n",
        "                rd_personnel.append(i)\n",
        "            if action & (1 << (i + NUM_PERSONNEL)):\n",
        "                pd_personnel.append(i)\n",
        "            if action & (1 << (i + 2 * NUM_PERSONNEL)):\n",
        "                cq_personnel.append(i)\n",
        "\n",
        "        return self.current_day, rd_personnel, pd_personnel, cq_personnel\n",
        "\n",
        "    def step(self, action):\n",
        "        day, rd_personnel, pd_personnel, cq_personnel = self._action_to_schedule(action)\n",
        "        reward = 0\n",
        "        done = False\n",
        "\n",
        "        # Validate action\n",
        "        valid = True\n",
        "\n",
        "        # 기본 검증\n",
        "        if len(rd_personnel) != 4:  # RD는 항상 4명\n",
        "            valid = False\n",
        "        if day in self.cq_schedule and set(cq_personnel) != set(self.cq_schedule[day]):\n",
        "            valid = False\n",
        "\n",
        "        # 중복 배정 검사\n",
        "        all_duty_personnel = set(rd_personnel) | set(pd_personnel) | set(cq_personnel)\n",
        "        if len(all_duty_personnel) != len(rd_personnel) + len(pd_personnel) + len(cq_personnel):\n",
        "            valid = False\n",
        "\n",
        "        if not valid:\n",
        "            return self.get_state(), -100, True\n",
        "\n",
        "        # 기록 저장\n",
        "        self.daily_duties.append((rd_personnel, pd_personnel, cq_personnel))\n",
        "\n",
        "        # Update personnel states\n",
        "        for p_id in range(NUM_PERSONNEL):\n",
        "            if p_id in self.leave_schedule.get(day, []):\n",
        "                self._grant_off(p_id)\n",
        "                continue\n",
        "\n",
        "            if p_id in rd_personnel:\n",
        "                self.personnel[p_id]['rd_count'] += 1\n",
        "                self._reset_consecutive_off(p_id)\n",
        "            elif p_id in pd_personnel:\n",
        "                self.personnel[p_id]['pd_count'] += 1\n",
        "                self._reset_consecutive_off(p_id)\n",
        "            elif p_id in cq_personnel:\n",
        "                self.personnel[p_id]['cq_next_day_off'] = True\n",
        "            else:\n",
        "                self._grant_off(p_id)\n",
        "\n",
        "        # Calculate rewards\n",
        "        reward += self._calculate_rewards()\n",
        "\n",
        "        self.current_day += 1\n",
        "        if self.current_day >= NUM_DAYS:\n",
        "            done = True\n",
        "            reward += self._final_validation()\n",
        "\n",
        "        return self.get_state(), reward, done\n",
        "\n",
        "    def _grant_off(self, p_id):\n",
        "        if self.personnel[p_id]['off_count'] < REQUIRED_OFF:\n",
        "            self.personnel[p_id]['off_count'] += 1\n",
        "            self.personnel[p_id]['last_off_days'] += 1\n",
        "        else:\n",
        "            # 12일 휴무를 채웠다면 PD 근무로 할당\n",
        "            self.personnel[p_id]['pd_count'] += 1\n",
        "            self.personnel[p_id]['last_off_days'] = 0\n",
        "\n",
        "    def _reset_consecutive_off(self, p_id):\n",
        "        if self.personnel[p_id]['cq_next_day_off']:\n",
        "            self._grant_off(p_id)\n",
        "            self.personnel[p_id]['cq_next_day_off'] = False\n",
        "        else:\n",
        "            self.personnel[p_id]['last_off_days'] = 0\n",
        "\n",
        "    def _calculate_rewards(self):\n",
        "        reward = 0\n",
        "\n",
        "        # 연속 휴무 관련 보상/패널티\n",
        "        for p in self.personnel.values():\n",
        "            if 1 < p['last_off_days'] <= MAX_CONSECUTIVE_OFF:\n",
        "                reward += p['last_off_days'] * 2\n",
        "            elif p['last_off_days'] > MAX_CONSECUTIVE_OFF:\n",
        "                reward -= 20\n",
        "\n",
        "            # PD 근무 분배 보상\n",
        "            if p['pd_count'] >= MIN_PD_DAYS:\n",
        "                reward += 5\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def _final_validation(self):\n",
        "        reward = 0\n",
        "\n",
        "        for p in self.personnel.values():\n",
        "            # 정확히 12일 휴무 보상/패널티\n",
        "            if p['off_count'] == REQUIRED_OFF:\n",
        "                reward += 50\n",
        "            else:\n",
        "                reward -= abs(p['off_count'] - REQUIRED_OFF) * 20\n",
        "\n",
        "            # 근무일수 합계 검증 (30일 = OFF + RD + PD + CQ후휴무)\n",
        "            total_days = p['off_count'] + p['rd_count'] + p['pd_count']\n",
        "            if p['cq_next_day_off']:\n",
        "                total_days += 1\n",
        "\n",
        "            if total_days != NUM_DAYS:\n",
        "                reward -= abs(NUM_DAYS - total_days) * 15\n",
        "\n",
        "            # 최소 PD 근무일수 검증\n",
        "            if p['pd_count'] < MIN_PD_DAYS:\n",
        "                reward -= (MIN_PD_DAYS - p['pd_count']) * 10\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def get_schedule_summary(self):\n",
        "        \"\"\"현재까지의 스케줄 요약 반환\"\"\"\n",
        "        summary = {\n",
        "            'personnel_stats': {\n",
        "                i: {\n",
        "                    'off_days': p['off_count'],\n",
        "                    'rd_days': p['rd_count'],\n",
        "                    'pd_days': p['pd_count'],\n",
        "                    'cq_next_off': p['cq_next_day_off']\n",
        "                } for i, p in self.personnel.items()\n",
        "            },\n",
        "            'daily_duties': self.daily_duties\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = DQN(state_size, action_size).to(self.device)\n",
        "        self.target_model = DQN(state_size, action_size).to(self.device)\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            q_values = self.model(state)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        states = torch.FloatTensor([t[0] for t in minibatch]).to(self.device)\n",
        "        actions = torch.LongTensor([t[1] for t in minibatch]).to(self.device)\n",
        "        rewards = torch.FloatTensor([t[2] for t in minibatch]).to(self.device)\n",
        "        next_states = torch.FloatTensor([t[3] for t in minibatch]).to(self.device)\n",
        "        dones = torch.FloatTensor([t[4] for t in minibatch]).to(self.device)\n",
        "\n",
        "        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q_values = self.target_model(next_states).max(1)[0]\n",
        "\n",
        "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
        "\n",
        "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "# 실행 예시\n",
        "if __name__ == \"__main__\":\n",
        "    cq_schedule = {5: [3], 15: [7]}\n",
        "    leave_schedule = {10: [2], 20: [5]}\n",
        "\n",
        "    env = DutySchedulerEnv(cq_schedule, leave_schedule)\n",
        "    state_size = len(env.reset())\n",
        "    action_size = 2 ** (NUM_PERSONNEL * 3)  # RD, PD, CQ 각각 10비트\n",
        "\n",
        "    agent = Agent(state_size, action_size)\n",
        "    batch_size = 64\n",
        "    episodes = 1000\n",
        "    target_update_frequency = 10\n",
        "\n",
        "    best_reward = float('-inf')\n",
        "    best_schedule = None\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "\n",
        "        for day in range(NUM_DAYS):\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if len(agent.memory) >= batch_size:\n",
        "                agent.replay(batch_size)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        if total_reward > best_reward:\n",
        "            best_reward = total_reward\n",
        "            best_schedule = env.get_schedule_summary()\n",
        "\n",
        "        if e % target_update_frequency == 0:\n",
        "            agent.update_target_model()\n",
        "\n",
        "        print(f\"Episode: {e+1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
        "\n",
        "    # 최적의 스케줄 출력\n",
        "    if best_schedule:\n",
        "        print(\"\\nBest Schedule Found:\")\n",
        "        for pid, stats in best_schedule['personnel_stats'].items():\n",
        "            print(f\"\\nPersonnel {pid}:\")\n",
        "            print(f\"  Off Days: {stats['off_days']}\")\n",
        "            print(f\"  RD Days: {stats['rd_days']}\")\n",
        "            print(f\"  PD Days: {stats['pd_days']}\")\n",
        "            print(f\"  CQ Next Off: {stats['cq_next_off']}\")\n",
        "            total_days = stats['off_days'] + stats['rd_days'] + stats['pd_days'] + int(stats['cq_next_off'])\n",
        "            print(f\"  Total Days: {total_days}\")"
      ],
      "metadata": {
        "id": "ZeFO3vGIWW6V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}